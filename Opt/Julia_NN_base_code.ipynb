{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ce94c4-4a0e-4d6a-b2a6-41884de7fd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "SYSTEM: caught exception of type :MethodError while trying to print a failed Task notice; giving up\n"
     ]
    }
   ],
   "source": [
    "using Plots\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a83db4-83ce-4187-ad27-48f24392927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(x) = 1.0 ./ (1.0 .+ exp.(-x))\n",
    "sigmoid_derivative(x) = #Implement derivative of sigmoid function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea1051-d1da-4080-affe-2256e0d87690",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct NeuralNetwork\n",
    "    inputSize::Int\n",
    "    hiddenSize::Int\n",
    "    outputSize::Int\n",
    "    \n",
    "    weightsLayerOne::Matrix{Float64}\n",
    "    weightsLayerTwo::Matrix{Float64}\n",
    "    \n",
    "    learningRate::Float64\n",
    "    optimiser::Symbol\n",
    " \n",
    "    function NeuralNetwork(inputSize::Int, hiddenSize::Int, outputSize::Int; \n",
    "                          learningRate::Float64=0.1,\n",
    "                          optimiser::Symbol=:gd)\n",
    "        \n",
    "        weightsLayerOne = randn(hiddenSize, inputSize) \n",
    "        weightsLayerTwo = randn(outputSize, hiddenSize) \n",
    "        \n",
    "        new(inputSize, hiddenSize, outputSize, \n",
    "            weightsLayerOne, weightsLayerTwo,\n",
    "            learningRate, optimiser)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb72a2a-ea02-4167-8c9d-b03caf5d3263",
   "metadata": {},
   "outputs": [],
   "source": [
    "function forward(self::NeuralNetwork, X::Matrix{Float64})\n",
    "\n",
    "    outputLayerOne = (self.weightsLayerOne * X)\n",
    "    activationLayerOne = sigmoid(outputLayerOne)\n",
    "\n",
    "    outputLayerTwo = (self.weightsLayerTwo * activationLayerOne) \n",
    "    activationLayerTwo = sigmoid(outputLayerTwo)\n",
    "    \n",
    "    return outputLayerOne, activationLayerOne, outputLayerTwo, activationLayerTwo\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6897589-520e-4fd6-9209-d46ba412905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(self::NeuralNetwork, X::Matrix{Float64})\n",
    "    _, _, _, activationLayerTwo = forward(self, X)\n",
    "    return activationLayerTwo\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8edfb2-1c69-4c0a-93db-6494c3fd56d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "function calculate_loss(self::NeuralNetwork, X::Matrix{Float64}, y::Matrix{Float64})\n",
    "    y_pred = predict(self, X)\n",
    "    return #Implement MSE loss function here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec50243-8957-49ce-bbf8-c3c53f96c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train!(self::NeuralNetwork, X::Matrix{Float64}, y::Matrix{Float64}, epochs::Int=15000)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in 1:epochs\n",
    "        \n",
    "        # Forward pass\n",
    "        outputLayerOne, activationLayerOne, outputLayerTwo, activationLayerTwo = forward(self, X)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = calculate_loss(self, X, y)\n",
    "        push!(losses, loss)\n",
    "        \n",
    "        # Implement backward pass based on partial derivative\n",
    "        \n",
    "\n",
    "        # Implement gradient descent, momentum, Adagrad and Adam\n",
    "        if self.optimiser == :gd\n",
    "            # Implement gradient descent here\n",
    "        elseif self.optimiser == :momentum\n",
    "            # Implement momentum here\n",
    "        elseif self.optimiser == :adagrad\n",
    "            # Implement adagrad here\n",
    "        elseif self.optimiser == :adam\n",
    "            # Implement Adam here\n",
    "        end\n",
    "            \n",
    "        if epoch % 1000 == 0\n",
    "            println(\"Epoch $epoch, Loss: $loss\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return losses\n",
    "end\n",
    "\n",
    "# XOR function input and output\n",
    "X = [[0.0,0.0]  [0.0,1.0]  [1.0,0.0] [1.0, 1.0]]\n",
    "y = [0.0 1.0 1.0 0.0]\n",
    "\n",
    "nn = NeuralNetwork(2, 5, 1, learningRate=0.1, optimiser=:gd)\n",
    "#nn = NeuralNetwork(2, 5, 1, learningRate=0.1, optimiser=:momentum)\n",
    "#nn = NeuralNetwork(2, 5, 1, learningRate=0.1, optimiser=:adagrad)\n",
    "#nn = NeuralNetwork(2, 5, 1, learningRate=0.1, optimiser=:adam)\n",
    "\n",
    "# Test the untrained model\n",
    "println(\"\\nResults before training:\")\n",
    "for i in 1:size(X, 2)\n",
    "    input = X[:, i:i] \n",
    "    prediction = predict(nn, input)\n",
    "    println(\"Input: [$(X[1,i]), $(X[2,i])], Predicted: $(round(prediction[1], digits=5)), Expected: $(y[i])\")\n",
    "end\n",
    "\n",
    "# Train the neural network and plot the loss\n",
    "losses = train!(nn, X, y, 15000)\n",
    "p = plot(1:length(losses), losses, \n",
    "         xlabel=\"Epoch\", \n",
    "         ylabel=\"Loss (MSE)\", \n",
    "         title=\"Neural Network Training Loss for XOR\", \n",
    "         legend=false, \n",
    "         lw=2,\n",
    "         yscale=:log10)\n",
    "display(p)\n",
    "\n",
    "# Test the trained model\n",
    "println(\"\\nResults after training:\")\n",
    "for i in 1:size(X, 2)\n",
    "    input = X[:, i:i]\n",
    "    prediction = predict(nn, input)\n",
    "    println(\"Input: [$(X[1,i]), $(X[2,i])], Predicted: $(round(prediction[1], digits=5)), Expected: $(y[i])\")\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.1",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
